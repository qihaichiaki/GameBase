### 关于敌人/NPC AI的强化学习


#### gamedemo
* 以本质来说, ai通过游戏程序基于它类似现实世界中的视觉、听觉(等感官), 将原本概率选择的状态替换为ai决策, 决策的要点是符合自身的最大利益.
* 以大黄蜂ai举例, 首先确定其符合自身的利益: 自身血量健康, 对玩家造成的伤害最大化。同时也要注意避免过于完美, 以维持游戏的趣味性和公平性

* 当前版本是纯普攻版本v1

##### 大黄蜂状态
1. Idle
   > idle 状态可以抵达其他的任意状态, 全凭当前的策略/环境所决定

2. Evade
   > evade躲闪状态, 
   > 可以轻巧的躲过玩家的攻击, 属于位移较小的躲避, 并且一直面向玩家
   > 闪避完后进入Idle状态, 如果碰到墙了会(尝试跳回去)->保留?

3. Jump
   > 可以用于躲闪玩家攻击, 靠近/远离玩家. 空中追击玩家. 空中可以进入瞄准状态(Fall同理), 发动dash攻击
   > 当垂直速度>=0时会进入Fall状态

4. Fall
   > 从空中到落地的过渡状态, 同样可以靠近/远离玩家，可以追击和瞄准玩家 
   > 如果大黄蜂掉落到未知领域(即坠入悬崖)， 当前的解决办法是朝着玩家方法进行大跳(大跳目前就这个用处), 后续也可以根据逻辑让ai不坠入悬崖

5. Dash
   > 地面冲刺会进入无敌状态, 短时间快速向前冲刺一段距离.
   > 可以快速贴近/远离 玩家, 快速贴近后可以迅速释放attack(中间不需要经过其他状态)
   > 碰到墙会停下或者尝试跳过去(释放保留?)
   > 如果冲出头了会跳回来
   > 正常情况下转换为Idle状态

6. AirDash
   > 和地面dash类似，这个是空中冲刺状态

7. Walk
   > 可以相对缓慢的靠近/远离玩家 
   > 可以保持距离， 获得更长时间的决策/观察玩家, 决定下次的状态

8. Attack
   > 存在三种普攻: 向上攻击，向下攻击，向前攻击
   > 攻击方式取决于玩家相对自身的位置, 一般玩家进入躲闪范围内概率触发
   > 有概率触发连续攻击

9. AttackBounce
   > 弹反, 在玩家攻击时释放此技能会触发一个向前大范围高额反弹伤害
   > 攻守兼备

10. DashAttackAim
    > 根据玩家的位置决定冲刺方式, 是冲刺攻击的大前摇, 冲刺方向和冲刺时间在前腰最后一帧决定

11. DashAttack
    > 在一段位移中持续的冲刺攻击, 可以让玩家无法攻击, 选择闪避格挡等策略



#### 状态中的决策
* 因为需要像人一样存在反应时间(不可能每一帧每一帧的进行决策), 所以每个状态下都会固定一个反应时间内执行决策, 此次决策是在对应状态下基于自身和环境以及玩家做出的下一次决策.
* 当前大黄蜂决策时间由`decisionMakingTime` 进行决策


#### 强化学习
* 深度强化学习(DRL)，训练的是一个: 状态-动作价值函数/策略网络。核心是通过(state，action，reward，nextState) 学习状态转移模式和奖励反馈
* 本质：“从当前状态出发，执行某个动作后，环境如何反馈（reward），并进入什么新状态（nextState）"


* 首先将ai之间的战斗细节进行拆分：
  1. 根据当前的状态，做出决策->行动
  2. 过渡到下一状态, 结算奖励

* 过渡过程中，可能会受到攻击，此时奖励程度就会得到修正, 所以一开始先填充做出这个动作期望的奖励


* 奖励机制:
  * 奖励目标: 让 AI 追求高血量，并尽量降低敌方血量
  * 鼓励ai造成伤害 鼓励ai规避伤害 惩罚ai受伤 惩罚ai站着不动浪费时间
  
* 奖励函数:
  1. 成功攻击玩家	+X  尽量攻击玩家
  2. 自己受伤 -X  尽量规避伤害
  3. 成功闪避 +1.5  尝试学会规避敌人攻击
  4. 什么都不做 -0.5  减少发呆时间，尝试主动进攻或防御

* 在DRL中，使用DQN算法来训练
DQN（Deep Q-Network）是深度强化学习（DRL）的一种算法，它属于值函数（Value-based）方法，专门用于解决离散动作空间的问题。

传统 Q-learning 使用Q 表格来存储状态-动作值（Q 值），但在复杂环境下状态空间太大，无法存储完整 Q 表。

DQN 使用神经网络（Deep Neural Network）来近似 Q 值，使其能够处理高维状态。